{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f535a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Control\n",
    "class EntryVehicle(object):\n",
    "    \n",
    "    '''\n",
    "    Defines an EntryVehicle class:\n",
    "    members:\n",
    "        area - the effective area, m^2\n",
    "        CD   - a multiplicative offset for the vehicle's drag coefficient\n",
    "        CL   - a multiplicative offset for the vehicle's lift coefficient\n",
    "        g0 - sea level Earth gravity, used in mass rate of change, m/s^2\n",
    "    methods:\n",
    "        aerodynamic_coefficients(Mach, alpha) - computes the values of CD and CL for the current Mach values, angle of attack\n",
    "        BC(mass, Mach) - computes the vehicle's ballistic coefficient as a function of its mass. Drag coefficient is calculated by default at Mach 24.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, area=0.4839, mass=907.2):\n",
    "        # need to put real value for cav thrust, ISP\n",
    "        self.area = area\n",
    "        self.mass = mass\n",
    "        self.g0 = 9.81\n",
    "\n",
    "    def aerodynamic_coefficients(self, M, alpha):\n",
    "        self.M = self.Control.mach()\n",
    "        self.alpha = self.Control.alpha()\n",
    "        from math import exp\n",
    "        \"\"\" Returns aero coefficients CD and CL. Supports ndarray Mach numbers. \"\"\"\n",
    "        [cl0, cl1, cl2, cl3] = [-0.2317, 0.0513, 0.2945, -0.1028]\n",
    "        [cd0, cd1, cd2, cd3] = [0.024, 7.24*exp(-4), 0.406, -0.323]\n",
    "        \n",
    "        cL = cl1*self.alpha + cl2*exp(cl3*self.M) + cl0\n",
    "        cD = cd1*self.alpha**2 + cd2*exp(cd3*self.M) + cd0\n",
    "        LoD = cL/cD\n",
    "        return cD, cL\n",
    "    \n",
    "    def ballistic_coefficients(self, mass=907.2, area=0.4839):\n",
    "        self.mass = mass\n",
    "        self.cD = self.aerodynamic_coefficients(M, alpha)[0]\n",
    "        self.area = area\n",
    "        \n",
    "        bc = mass/(self.cD*area)\n",
    "        return bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c79f71ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:22: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:22: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14928/1202008157.py:22: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if model is 'exp':\n"
     ]
    }
   ],
   "source": [
    "import isacalc as isa\n",
    "atmos = isa.get_atmosphere\n",
    "\n",
    "class Planet:\n",
    "    def __init__(self, name='Earth', rho0=0, scaleHeight=0, model='exp', da=False):\n",
    "\n",
    "        self.name = name.capitalize()\n",
    "        self._da = da  # Differential algebraic inputs\n",
    "\n",
    "        if self.name == 'Earth':\n",
    "            self.radius = 6378.1e3\n",
    "            self.omega = 7.292115e-5\n",
    "            self.mu = 3.98600e14\n",
    "            \n",
    "            self.atmosphere= self.__atmos_earth\n",
    "\n",
    "        elif self.name == 'Mars':\n",
    "            self.radius = 3396.2e3\n",
    "            self.omega = 7.095e-5\n",
    "            self.mu = 4.2830e13\n",
    "\n",
    "            if model is 'exp':\n",
    "                self.rho0 = (1+rho0)*0.0158\n",
    "                self.scaleHeight = (1+scaleHeight)*9354.5\n",
    "                self.atmosphere = self.__exp_model_mars\n",
    "\n",
    "            else:\n",
    "                # Sample MG and store interpolators for density and speed of sound\n",
    "                self.atmosphere = self.__MG_model_mars\n",
    "        else:\n",
    "            print('Input planet name, '+ self.name +', is not valid')\n",
    "    def __atmos_earth(self, h):\n",
    "        atmosphere = isa.get_atmosphere\n",
    "        T, P, rho, a, mu = isa.calculate_at_h(h, atmosphere)\n",
    "        return rho, a\n",
    "        \n",
    "    def __exp_model_mars(self, h):\n",
    "        ''' Defines an exponential model of the atmospheric density and local speed of sound as a function of altitude. '''\n",
    "        if self._da:\n",
    "            from pyaudi import exp\n",
    "            scalar=False\n",
    "            try:\n",
    "                h[0]\n",
    "            except:\n",
    "                scalar=True\n",
    "                h = [h]\n",
    "            #Density computation:\n",
    "            rho = [self.rho0*exp(-hi/self.scaleHeight) for hi in h]\n",
    "\n",
    "            # Local speed of sound computation:\n",
    "            coeff = [223.8, -0.2004e-3, -1.588e-8, 1.404e-13]\n",
    "            a = [sum([c*hi**i for i,c in enumerate(coeff)]) for hi in h]\n",
    "            if scalar:\n",
    "                a = a[0]\n",
    "                rho = rho[0]\n",
    "\n",
    "        else:\n",
    "            from numpy import exp\n",
    "            #Density computation:\n",
    "            rho = self.rho0*exp(-h/self.scaleHeight)\n",
    "\n",
    "            # Local speed of sound computation:\n",
    "            coeff = [223.8, -0.2004e-3, -1.588e-8, 1.404e-13]\n",
    "            a = sum([c*h**i for i,c in enumerate(coeff)])\n",
    "\n",
    "        return rho,a\n",
    "\n",
    "    def __MG_model_mars(self, h):\n",
    "        ''' Interpolates data from an MG profile '''\n",
    "        return self.density(h),self.speed_of_sound(h)\n",
    "\n",
    "    def updateMG(date=[10,29,2018], latitude=0, longitude=0, dustTau=0, rpscale=0):\n",
    "        ''' Calls MG '''\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a478fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sin, cos, tan\n",
    "#from Entry_Vehicle import EntryVehicle\n",
    "#import Planet\n",
    "\n",
    "class Entry(object):\n",
    "    \"\"\"  Basic equations of motion for unpowered and powered flight through an atmosphere. \"\"\"\n",
    "\n",
    "    def __init__(self, PlanetModel=Planet('Earth'), VehicleModel=EntryVehicle(), Coriolis=False, Powered=False, Energy=False, Altitude=False, DifferentialAlgebra=False, Scale=False, Longitudinal=False, Velocity=False):\n",
    "\n",
    "\n",
    "        \n",
    "        self.planet = PlanetModel\n",
    "        self.vehicle = VehicleModel\n",
    "        self.powered = Powered\n",
    "        self.drag_ratio = 1\n",
    "        self.lift_ratio = 1\n",
    "        self.nx = 6  # [r,lon,lat,v,gamma,psi]\n",
    "        self.nu = 2  # bank command, angle of attack\n",
    "        self.__jacobian = None  # If the jacobian method is called, the Jacobian object is stored to prevent recreating it each time. It is not constructed by default.\n",
    "        self.__jacobianb = None\n",
    "        self._da = DifferentialAlgebra\n",
    "        self.planet._da = DifferentialAlgebra\n",
    "\n",
    "        #todo: Non-dimensionalizing the states\n",
    "        if Scale:\n",
    "            self.dist_scale = self.planet.radius\n",
    "            self.acc_scale = self.planet.mu/(self.dist_scale**2)\n",
    "            self.time_scale = np.sqrt(self.dist_scale/self.acc_scale)\n",
    "            self.vel_scale = np.sqrt(self.dist_scale*self.acc_scale)\n",
    "            self.long_scale = self.theta_us/pi\n",
    "            self.lat_scale = self.phi_us/pi\n",
    "            self.fpa_scale = self.gamma_us/pi\n",
    "            self.ha_scale = self.psi_us/pi\n",
    "            self.mass_scale = 1\n",
    "\n",
    "        else:  # No scaling\n",
    "            self.dist_scale = 1\n",
    "            self.acc_scale = 1\n",
    "            self.time_scale = 1\n",
    "            self.vel_scale = 1\n",
    "        \n",
    "\n",
    "        self.dyn_model = self.__entry_3dof\n",
    "\n",
    "    def update_ratios(self, LR, DR):\n",
    "        self.drag_ratio = DR\n",
    "        self.lift_ratio = LR\n",
    "\n",
    "    def DA(self, bool=None):\n",
    "        if bool is None:\n",
    "            return self._da\n",
    "        else:\n",
    "            self._da = bool\n",
    "            self.planet._da = bool\n",
    "\n",
    "# Dynamics model\n",
    "        \n",
    "    def __entry_3dof(self, s, alpha, sigma, t):\n",
    "        r, theta, phi, v, gamma, psi =  self.s\n",
    "        g = self.gravity(r)\n",
    "        omega = self.planet.omega\n",
    "        h = (r - self.planet.radius)/self.dist_scale\n",
    "        rho, a = self.planet.atmosphere(h*self.dist_scale)\n",
    "        M = v*self.vel_scale/s_s\n",
    "        alpha = self.Control.alpha\n",
    "        sigma = self.Control.sigma\n",
    "        cD, cL = self.vehicle.aerodynamic_coefficients(M, alpha)\n",
    "        m = self.vehicle.mass\n",
    "        f = np.squeeze(0.5*rho*self.vehicle.area*v**2/m)*self.dist_scale  # vel_scale**2/acc_scale = dist_scale \n",
    "        L = f*cL*self.lift_ratio\n",
    "        D = f*cD*self.drag_ratio      \n",
    "        \n",
    "        dr = v*sin(gamma)\n",
    "        dtheta = v*cos(gamma)*cos(psi)/r*cos(phi)\n",
    "        dphi = v*cos(gamma)*sin(psi)/r\n",
    "        dv = -D/m - g*sin(gamma) + omega**2*r*cos(phi)*(sin(gamma)*cos(phi) - cos(gamma)*sin(phi)*sin(psi))\n",
    "        dgamma = L*cos(sigma)/m*v + cos(gamma)*((v**2-g*r)/r*v) + 2*omega*sin(psi)*cos(phi) + omega**2*r*cos(phi)*(cos(phi)*cos(gamma)+sin(phi)*sin(psi)*sin(gamma)) \n",
    "        dpsi = L*sin(sigma)/m*v*cos(gamma) - v*cos(gamma)*cos(psi)*tan(phi)/r + 2*omega*(tan(gamma)*cos(phi)*sin(psi)-sin(phi)) - omega**2*r*sin(phi)*cos(phi)*cos(psi)/cos(gamma)\n",
    "\n",
    "        return np.array([dr, dtheta, dphi, dv, dgamma, dpsi])\n",
    "    \n",
    "    def aeroforces(self, r, v, m):\n",
    "        \"\"\"  Returns the aerodynamic forces acting on the vehicle at a given radius, velocity and mass. \"\"\"\n",
    "\n",
    "        h = r - self.planet.radius\n",
    "        rho, a = self.planet.atmosphere(h)\n",
    "        M = v/a\n",
    "        # cD, cL = self.vehicle.aerodynamic_coefficients(M)\n",
    "        cD, cL = self.vehicle.aerodynamic_coefficients(v)\n",
    "        f = 0.5*rho*self.vehicle.area*v**2/m\n",
    "        L = f*cL*self.lift_ratio\n",
    "        D = f*cD*self.drag_ratio\n",
    "        return L, D\n",
    "\n",
    "    def gravity(self, r):\n",
    "        \"\"\" Returns gravitational acceleration at a given planet radius based on quadratic model \n",
    "        \n",
    "            For radius in meters, returns m/s**2\n",
    "            For non-dimensional radius, returns non-dimensional gravity \n",
    "        \"\"\"\n",
    "        return self.planet.mu/(r*self.dist_scale)**2/self.acc_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd711686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\User')\n",
    "from scipy import cos\n",
    "import pybrain\n",
    "from pybrain.rl.environments.episodic import EpisodicTask\n",
    "#import Entry_Vehicle\n",
    "#import Entry\n",
    "#import Planet\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class Control(EpisodicTask):\n",
    "\n",
    "    # number of steps of the current trial\n",
    "    steps = 0\n",
    "\n",
    "    # number of the current episode\n",
    "    episode = 0\n",
    "    \n",
    "    maxSteps = 99\n",
    "    \n",
    "    resetOnSuccess = True\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.cumreward = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.GetInitialState()\n",
    "        alpha = random.uniform(-20, 20)\n",
    "        sigma = 0\n",
    "    \n",
    "    def getObservation(self):    \n",
    "        self.state = self.Entry.__entry_3dof(self, s, alpha, sigma, t)\n",
    "        return [(self.state)]\n",
    "        \n",
    "    def performAction(self, action):\n",
    "        if self.done > 0:\n",
    "            self.done += 1            \n",
    "        else:\n",
    "            self.state = self.DoAction(action, self.state)\n",
    "            self.r, self.done = self.GetReward(self.state, action)\n",
    "            self.cumreward += self.r\n",
    "            \n",
    "    def getReward(self):\n",
    "        return self.r    \n",
    "\n",
    "    def GetInitialState(self):\n",
    "        # Initialize state values\n",
    "        self.StartEpisode()\n",
    "        r = 6448\n",
    "        theta = 0\n",
    "        phi = 0\n",
    "        v = 6500\n",
    "        gamma = 0\n",
    "        psi = 90\n",
    "        \n",
    "        return [r, theta, phi, v, gamma, psi]\n",
    "  \n",
    "    def FinalState(self):\n",
    "        # set final target values\n",
    "        r = 6408\n",
    "        theta = 50\n",
    "        phi = 0\n",
    "        v = 1600\n",
    "        gamma = 0\n",
    "        psi = 90\n",
    "        \n",
    "        return [r, theta, phi, v, gamma, psi]\n",
    "    \n",
    "    def StartEpisode(self):\n",
    "        self.steps = 0\n",
    "        self.episode = self.episode + 1\n",
    "        self.done = 0\n",
    "        \n",
    "    def isFinished(self):\n",
    "        if self.done>=1 and self.resetOnSuccess:\n",
    "            self.reset()\n",
    "            return True\n",
    "        else:\n",
    "            return self.done>=3\n",
    "    \n",
    "\n",
    "    def GetReward(self, s, a):\n",
    "        #todo: formulate reward function\n",
    "        # satisfy constraints either here or in doaction\n",
    "        r = 0\n",
    "        f = 0\n",
    "        \n",
    "        if (self.s-self.FinalState) == np.zeros(6):\n",
    "            r = 100\n",
    "        elif np.absolute(self.s-self.FinalState) < np.array([10, 5, 5, 100, 5, 5]):\n",
    "            r = 50\n",
    "        elif np.absolute(self.s-self.FinalState) < np.array([20, 10, 10, 400, 10, 10]):\n",
    "            r = 10\n",
    "        elif np.absolute(self.s-self.FinalState) < np.array([30, 20, 20, 1000, 20, 20]):\n",
    "            r= 2\n",
    "        elif np.absolute(self.s-self.FinalState) < np.array([40, 30, 30, 2000, 30, 30]):\n",
    "            r = 0 \n",
    "        elif np.absolute(self.s-self.FinalState) > np.array([40, 30, 30, 2000, 30, 30]):\n",
    "            r = -1\n",
    "        elif np.absolute(self.s-self.FinalState) > np.array([45, 30, 30, 2500, 30, 30]):\n",
    "            r = -5\n",
    "        elif np.absolute(self.s-self.FinalState) > np.array([50, 45, 45, 3000, 45, 45]):\n",
    "            r = -10\n",
    "        if self.steps >= self.maxSteps:\n",
    "            f = 5\n",
    "   \n",
    "        return r, f\n",
    "\n",
    "    def DoAction(self, a, s):\n",
    "\n",
    "        alpha = self.alpha\n",
    "        sigma = self.sigma\n",
    "        # todo: satisfy constraints\n",
    "        self.steps = self.steps + 1\n",
    "        s_new = s + self.Entry.__entry_3dof(s, alpha, sigma, t)\n",
    "\n",
    "        \n",
    "        return [s_new]\n",
    "    \n",
    "    def mach(self):\n",
    "        M = self.Entry.__entry_3dof.M\n",
    "        \n",
    "        return M\n",
    "    \n",
    "    def alpha(self):\n",
    "        #alpha = self.get_action[0]\n",
    "        ''''\n",
    "        if self.step=0:\n",
    "            alpha = random.uniform(-20, 20)\n",
    "        else:\n",
    "            pass\n",
    "            '''\n",
    "        alpha_old = self.AlphaOld\n",
    "        alpha_new = random.uniform(-20, 20)\n",
    "        dalpha = alpha_new - alpha_old\n",
    "        \n",
    "        if dalpha<-5:\n",
    "            alpha = alpha_old - 5\n",
    "        elif dalpha>5:\n",
    "            alpha = alpha_old + 5\n",
    "        else:\n",
    "            alpha = alpha_new\n",
    "        return alpha\n",
    "    \n",
    "    def AlphaOld(self):\n",
    "        alpha_old = self.alpha\n",
    "        return alpha_old\n",
    "    \n",
    "    def sigma(self):\n",
    "        #sigma = self.get_action[1]\n",
    "        '''  \n",
    "        if self.step=0:\n",
    "            sigma = 0\n",
    "        else:\n",
    "            pass\n",
    "        '''\n",
    "        sigma_old = self.SigmaOld\n",
    "        sigma_new = random.uniform(-90, 90)\n",
    "        dsigma = sigma_new - sigma_old\n",
    "        \n",
    "        if dsigma<-15:\n",
    "            sigma = sigma_old - 15\n",
    "        elif dsigma>15:\n",
    "            sigma = sigma_old + 15\n",
    "        else:\n",
    "            sigma = sigma_new\n",
    "        return sigma\n",
    "    \n",
    "    def SigmaOld(self):\n",
    "        sigma_old = self.sigma\n",
    "        return sigma_old\n",
    "    \n",
    "    def contraints(self):\n",
    "        V = self.Entry.__entry_3dof[3]\n",
    "        K = 9.289e-9\n",
    "        rho = self.Planet.atmosphere[0]\n",
    "        Cd, Cl = self.EntryVehicle.aerodynamic_coefficients(M, alpha)\n",
    "        D = 0.5 * rho * V **2 * S * Cd\n",
    "        L = (Cl/Cd) * D\n",
    "        #aerodynamic heating\n",
    "        Qd = K * rho ** 0.5 * V ** 3\n",
    "        #dynamic pressure\n",
    "        P_d = 0.5 * rho * V ** 2\n",
    "        #normal load\n",
    "        n_L = math.sqrt(L**2+D**2)/(self.mass*self.gravity)\n",
    "        return [[Qd, P_d, n_L]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd992da9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14928/1098994799.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mHrvEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEnv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHrv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoiseRange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Env' is not defined"
     ]
    }
   ],
   "source": [
    "class HrvEnv(Env):\n",
    "    print_interval = 1\n",
    "    def __init__(self, logger = None):\n",
    "        self.env = Hrv()\n",
    "        self.noiseRange = 1.0\n",
    "        self.noiseMax = 1.0\n",
    "        self.om = 0\n",
    "        self.a = 0.6\n",
    "        self.b = 0.4\n",
    "        self.t = 0\n",
    "        self.totStep = 0\n",
    "        self.r = 0\n",
    "        self.ep = 0\n",
    "        if logger==None:\n",
    "            self.perfs = result_log(algo=\"DDPG\", l1=20, l2=10)\n",
    "        else:\n",
    "            self.perfs = logger\n",
    "        self.actif = True\n",
    "        #self.plot = result_plot()\n",
    "    \n",
    "    def state(self):\n",
    "        return [self.env.getObservation()]\n",
    "    def act(self, action):\n",
    "        actNoise = action + self.noise_func()\n",
    "        self.env.performAction(actNoise[0])\n",
    "        r = self.env.getReward()\n",
    "        self.t += 1\n",
    "        self.r += r\n",
    "        return actNoise, [r]\n",
    "    def reset(self, noise=True):\n",
    "        self.actif = True\n",
    "        self.env.reset()\n",
    "        self.om = 0\n",
    "        self.totStep+=self.t\n",
    "        if self.totStep != 0:\n",
    "            self.perfs.addData(self.totStep, self.t, self.r)\n",
    "        self.t = 0\n",
    "        self.r = 0\n",
    "        self.ep += 1\n",
    "        if not noise:\n",
    "            self.noiseRange = 0.0\n",
    "        else:\n",
    "            self.noiseRange = random.uniform(0.,self.noiseMax)\n",
    "    def noise_func(self):\n",
    "        self.om = self.om-self.a*self.om + self.b*random.gauss(0,1)*self.noiseRange\n",
    "        return self.om\n",
    "    def isFinished(self):\n",
    "        if self.actif and not self.env.isFinished():\n",
    "            return False\n",
    "        else:\n",
    "            self.actif = False\n",
    "            return True\n",
    "    def getActionSize(self):\n",
    "        return 2\n",
    "    def getStateSize(self):\n",
    "        return 6\n",
    "    def getActionBounds(self):\n",
    "        return np.array[[-20], [20],\n",
    "               [-90], [90]]\n",
    "    def printEpisode(self):\n",
    "        print(time.strftime(\"[%H:%M:%S]\"), \" Episode : \" , self.ep, \" steps : \", self.t, \" reward : \", self.r, \"noise : \", self.noiseRange)\n",
    "    def performances(self):\n",
    "        pass#self.plot.clear()\n",
    "        #self.plot.add_row(self.perfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d0eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\User')\n",
    "import DDPG\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "from DDPG.core.networks.simple_actor_network import simple_actor_network\n",
    "from DDPG.core.networks.simple_critic_network import simple_critic_network\n",
    "from DDPG.core.helper.tensorflow_grad_inverter import grad_inverter\n",
    "from DDPG.environement.env import Env\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"A définir\"\"\"\n",
    "def void_noise_func(t):\n",
    "    return 0.0\n",
    "def void_env_act(action, store):\n",
    "    return 0\n",
    "def void_env_state():\n",
    "    return [0]\n",
    "def void_env_ini():\n",
    "    pass\n",
    "def void_env_draw():\n",
    "    pass\n",
    "def void_env_stop_signial():\n",
    "    return False\n",
    "\n",
    "class DDPG(object):\n",
    "    \"\"\"DDPG's main structure implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, env, actor = None, critic = None):\n",
    "        self.env = env\n",
    "        if not isinstance(env,Env):\n",
    "            print (\"error\")\n",
    "        \n",
    "        s_dim = env.getStateSize()\n",
    "        a_dim = env.getActionSize()\n",
    "        action_bounds = env.getActionBounds()\n",
    "        \n",
    "        if actor == None:\n",
    "            self.actor = simple_actor_network(s_dim, a_dim)\n",
    "        else:\n",
    "            self.actor = actor\n",
    "        if critic == None:\n",
    "            self.critic = simple_critic_network(s_dim, a_dim, action_bounds)\n",
    "        else:\n",
    "            self.critic = critic \n",
    "        self.buffer = deque([])\n",
    "        self.buffer_size = 100000\n",
    "        self.buffer_minimum = 100\n",
    "        self.minibatch_size = 64\n",
    "        self.t = 0\n",
    "        \n",
    "        self.grad_inv = grad_inverter(action_bounds)\n",
    "        \n",
    "        self.train_loop_size = 1\n",
    "        self.totStepTime = 0\n",
    "        self.totTrainTime = 0\n",
    "        self.batchMix = 0\n",
    "        self.calcTrain = 0\n",
    "        self.time0 = 0\n",
    "        self.time1 = 0\n",
    "        self.time2 = 0\n",
    "        self.numStep = 0\n",
    "        self.stepsTime = 0\n",
    "    \n",
    "    def react(self, state):\n",
    "        act = self.actor.action_batch(state)\n",
    "        return act\n",
    "        \n",
    "    def store_transition(self, s_t, a_t, r_t, s_t_nxt):\n",
    "        for i in range(len(s_t)):\n",
    "            if(len(self.buffer)>=self.buffer_size):\n",
    "                self.buffer[random.randint(self.buffer_size/5, self.buffer_size-1)] = [list(s_t[i]), list(a_t[i]), r_t[i], list(s_t_nxt[i])]\n",
    "            else:\n",
    "                self.buffer.append([list(s_t[i]), list(a_t[i]), r_t[i], list(s_t_nxt[i])])\n",
    "        \n",
    "            \n",
    "    def train_Minibatch(self):\n",
    "        if(len(self.buffer)>self.buffer_minimum):\n",
    "            rewards = []\n",
    "            states = []\n",
    "            nxt_states = []\n",
    "            actions_batch = []\n",
    "            batchMix = time.time()\n",
    "            for i in range(self.minibatch_size):\n",
    "                r= random.randint(0, len(self.buffer)-1)\n",
    "                tmp = self.buffer[r]\n",
    "                rewards.append([tmp[2]])\n",
    "                states.append(tmp[0])\n",
    "                nxt_states.append(tmp[3])\n",
    "                actions_batch.append(tmp[1])\n",
    "            self.batchMix+=time.time()-batchMix\n",
    "            \n",
    "            calcTrain = time.time()\n",
    "            y = self.critic.y_val_calc(rewards, self.critic.q_val_batch(nxt_states, self.actor.action_batch(nxt_states, True), True))\n",
    "            self.critic.batch(states, actions_batch, y)\n",
    "            self.critic.update()\n",
    "            self.time0 += time.time() - calcTrain\n",
    "            \n",
    "            time1 = time.time()\n",
    "            actions = self.actor.action_batch(states)\n",
    "            actionGradients = self.grad_inv.invert(self.critic.actionGradient_batch(states, actions), actions)\n",
    "            self.actor.batch(states, actionGradients)\n",
    "            self.actor.update()\n",
    "            self.time1 += time.time() -time1\n",
    "            \n",
    "            time2 = time.time()\n",
    "            self.critic.updateTarget()\n",
    "            self.actor.updateTarget()\n",
    "            self.time2+= time.time() -time2\n",
    "            self.calcTrain+= time.time() - calcTrain\n",
    "            \n",
    "    def step(self, train):\n",
    "        state = list(self.env.state())\n",
    "        action = self.react(state)\n",
    "        (action, reward) = self.env.act(action)\n",
    "        if train:\n",
    "            self.store_transition(state, action, reward, self.env.state())\n",
    "        self.t += 1\n",
    "        return reward\n",
    "    \n",
    "    def episode(self, max_t=float(\"inf\"), train=True):\n",
    "        while self.t<max_t and not self.env.isFinished():\n",
    "            stepTime = time.time()\n",
    "            self.step(train)\n",
    "            if train :\n",
    "                self.totStepTime += time.time() - stepTime\n",
    "                for i in range(self.train_loop_size):\n",
    "                    trainTime = time.time()\n",
    "                    self.train_Minibatch()\n",
    "                    self.totTrainTime += time.time() - trainTime\n",
    "                self.numStep+=1\n",
    "    def M_episodes(self, M, T=float(\"inf\"), train=True):\n",
    "        for i in range(M):\n",
    "            self.t = 0\n",
    "            if self.env.isFinished():\n",
    "                self.env.reset()\n",
    "            self.episode(T, train)\n",
    "            if i % self.env.print_interval == 0:\n",
    "                self.stepsTime += self.totStepTime + self.totTrainTime\n",
    "                self.env.printEpisode()\n",
    "                self.env.draw()\n",
    "                #print \"step time : \", self.totStepTime/(self.totStepTime+self.totTrainTime) , \"train time : \", self.totTrainTime/(self.totStepTime+self.totTrainTime)\n",
    "                #print \"train decomp -> batch mix : \", self.batchMix/(self.batchMix+self.calcTrain), \" calculations : \", self.calcTrain/(self.batchMix+self.calcTrain)\n",
    "                #print \"time 0 : \", self.time0/(self.time0 + self.time1+self.time2), \"time 1 : \", self.time1/(self.time0 + self.time1+self.time2), \"time 2 : \", self.time2/(self.time0 + self.time1+self.time2)                \n",
    "                print (\"Steps/minutes : \" , 60.0 / (self.stepsTime / self.numStep))             \n",
    "                self.totStepTime = 0\n",
    "                self.totTrainTime = 0\n",
    "                self.batchMix = 0\n",
    "                self.calcTrain = 0\n",
    "                self.time0 = 0\n",
    "                self.time1 = 0\n",
    "                self.time2 = 0\n",
    "#            if i % 1 == 10:\n",
    "#                self.env.reset(noise=False)\n",
    "#                self.episode(T, train=False)\n",
    "#                #self.stepsTime += self.totStepTime + self.totTrainTime\n",
    "#                self.env.printEpisode()\n",
    "#                self.env.draw()\n",
    "#                #print \"Steps/minutes : \" , 60.0 / (self.stepsTime / self.numStep)                \n",
    "#                self.totStepTime = 0\n",
    "#                self.totTrainTime = 0\n",
    "#                self.batchMix = 0\n",
    "#                self.calcTrain = 0\n",
    "#                self.time0 = 0\n",
    "#                self.time1 = 0\n",
    "#                self.time2 = 0\n",
    "        if self.env.isFinished():\n",
    "            self.env.reset()\n",
    "    def buffer_flush(self):\n",
    "        self.buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88863678",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tf' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14928/3412859070.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMountainCarEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DDPG\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0ma_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_actor_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvoidFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DDPG\\core\\networks\\simple_actor_network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, state_size, action_size, l1_size, l2_size, learning_rate)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"A first actor network for low-dim state\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml1_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'tf' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from DDPG.core.DDPG_core import DDPG\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import DDPG.environement.instance.mountainCarEnv as mc\n",
    "from DDPG.core.networks.simple_actor_network import simple_actor_network\n",
    "from DDPG.logger.result import result_log\n",
    "\n",
    "l1 = 20\n",
    "l2 = 10\n",
    "rate = 0.001\n",
    "\n",
    "env = mc.MountainCarEnv(result_log(\"DDPG\", l1, l2))\n",
    "a_c = DDPG(env, actor = simple_actor_network(2, 1, l1_size = l1, l2_size = l2, learning_rate = rate))\n",
    "\n",
    "def voidFunc():\n",
    "    pass\n",
    "\n",
    "env.extern_draw = voidFunc\n",
    "\n",
    "def doEp(M, T=float(\"inf\")):\n",
    "    a_c.M_episodes(M, T)\n",
    "    env.perfs.save()\n",
    "doEp(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b00915e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tf' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14928/3225309262.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMountainCarEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0ma_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_actor_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_critic_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdraw_politic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DDPG\\core\\networks\\simple_actor_network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, state_size, action_size, l1_size, l2_size, learning_rate)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"A first actor network for low-dim state\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml1_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'tf' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from DDPG.core.DDPG_core import DDPG\n",
    "import numpy as np\n",
    "\n",
    "import DDPG.environement.instance.mountainCarEnv as mc\n",
    "from DDPG.core.networks.simple_actor_network import simple_actor_network\n",
    "from DDPG.core.networks.simple_critic_network import simple_critic_network\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from DDPG.logger.result import result_log\n",
    "\n",
    "l1 = 20\n",
    "l2 = 10\n",
    "\n",
    "logger = result_log(\"DDPG\", l1, l2, \"simple_\"+str(l1)+\"_\"+str(l2))\n",
    "\n",
    "env = mc.MountainCarEnv(logger)\n",
    "a_c = DDPG(env, actor = simple_actor_network(2, 1, l1_size = l1, l2_size = l2, learning_rate = 0.005), critic = simple_critic_network(2, 1, l1_size = 20, l2_size = 10, learning_rate = 0.01))\n",
    "    \n",
    "def draw_politic():\n",
    "    plt.close()\n",
    "    ac= a_c\n",
    "    img = np.zeros((200, 200))\n",
    "    pos = -1.\n",
    "    batch = []\n",
    "    for i in range(200):\n",
    "        vel = -1.\n",
    "        pos += 0.01\n",
    "        for j in range(200):\n",
    "            vel += 0.01\n",
    "            batch.append([pos, vel])\n",
    "    pol = ac.react(batch)\n",
    "    b=0           \n",
    "    print(\"politic max : \", max(pol), \" politic min : \", min(pol))\n",
    "    for i in range(200):\n",
    "        for j in range(200):\n",
    "            img[-j][i] = max(-1, min(1.0, pol[b]))\n",
    "            b += 1\n",
    "    img[0][0] = -1\n",
    "    img[-1][-1] = 1.0\n",
    "    plt.imshow(img, extent=(-1.0,1.0,-1.0,1.0))\n",
    "    plt.show(block=False)\n",
    "def draw_episode():\n",
    "    act = a_c.actor\n",
    "    env.reset()\n",
    "    env.noiseRange=0.0\n",
    "    while not env.isFinished():\n",
    "        plt.scatter((env.state()[0][0]),(env.state()[0][1]), c=\"white\")\n",
    "        env.act(act.action_batch(env.state()))\n",
    "\n",
    "def perfs():\n",
    "    env.performances()\n",
    "\n",
    "def voidFunc():\n",
    "    pass\n",
    "\n",
    "env.extern_draw = voidFunc\n",
    "def draw_buffer():\n",
    "    for i in range(len(a_c.buffer)):\n",
    "        plt.scatter((a_c.buffer[i][0][0]+1.)*100, (a_c.buffer[i][0][1]+1.)*100)\n",
    "\n",
    "def doEp(M, T=float(\"inf\")):\n",
    "    a_c.M_episodes(M, T)\n",
    "    env.perfs.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6dcbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f6f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09eb812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
